---
File: .github/workflows/ci.yml
---
# .github/workflows/ci.yml - Continuous Integration workflow

name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

permissions:
  contents: write  # Allow pushing formatted code back

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Lint with flake8
      run: |
        flake8 obsidian/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 obsidian/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics

    - name: Format code with black
      run: |
        black obsidian/ tests/

    - name: Sort imports with isort
      run: |
        isort obsidian/ tests/

    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        git diff --staged --quiet || git commit -m "Auto-format code with black and isort"
        git push || echo "No changes to push"

    - name: Type check with mypy (non-blocking)
      run: |
        mypy obsidian/ || echo "⚠️ MyPy found issues, but continuing build"
      continue-on-error: true

    - name: Test with pytest
      run: |
        pytest --cov=obsidian --cov-report=xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  build:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: python -m build

    - name: Check distribution
      run: twine check dist/*

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist
        path: dist/



---
File: .github/workflows/publish.yml
---
# .github/workflows/publish.yml - PyPI publishing workflow

name: Publish to PyPI

on:
  release:
    types: [published]

jobs:
  publish:
    runs-on: ubuntu-latest
    environment: pypi
    permissions:
      id-token: write  # For trusted publishing
      contents: read

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: python -m build

    - name: Check distribution
      run: twine check dist/*

    # - name: Publish to Test PyPI
    #   uses: pypa/gh-action-pypi-publish@release/v1
    #   with:
    #     repository-url: https://test.pypi.org/legacy/
    #     skip-existing: true
    #   env:
    #     TWINE_USERNAME: __token__
    #     TWINE_PASSWORD: ${{ secrets.TEST_PYPI_API_TOKEN }}

    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}



---
File: .github/workflows/summary-for-llm.yml
---
name: Llamero Summarization

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  generate-summaries:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Install llamero
      run: touch requirements.txt

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        #cache: 'pip'

    - name: Install llamero
      run: pip install llamero

    - name: Generate summaries
      run: |
        llamero summarize all
        #llamero tree --output summaries/tree.md



---
File: README.md
---
# dtd-obsidian-tools
misc tools for working with obsidian vaults



---
File: obsidian/__init__.py
---
# obsidian/__init__.py - Basic package initialization

from .graph import build_graph, find_candidates, get_link_statistics, load_corpus
from .parser import ObsDoc, clean_links, extract_frontmatter, get_wikilinks, read_yaml

__all__ = [
    "ObsDoc",
    "read_yaml",
    "extract_frontmatter",
    "get_wikilinks",
    "clean_links",
    "load_corpus",
    "build_graph",
    "get_link_statistics",
    "find_candidates",
]



---
File: obsidian/colorclass_processor.py
---
# obsidian/colorclass_processor.py - Add unique colorclass tags with community detection

import colorsys
import json
import random
from collections import Counter
from pathlib import Path
from typing import TYPE_CHECKING, Any, Tuple

import fire
import frontmatter
import networkx as nx
import seaborn as sns
from loguru import logger
from omegaconf import DictConfig, OmegaConf

if TYPE_CHECKING:
    from .parser import ObsDoc
else:
    # Runtime imports to avoid circular dependencies
    pass

from .graph import build_graph, find_candidates, get_link_statistics, load_corpus

def hsl_to_rgb_int(hsl: Tuple[float, float, float]) -> int:
    """
    Convert HSL color to RGB integer format used by Obsidian.
    
    Args:
        hsl: Tuple of (hue, saturation, lightness) values between 0-1
        
    Returns:
        RGB integer value
    """
    h, s, l = hsl
    r, g, b = colorsys.hls_to_rgb(h, l, s)  # Note: colorsys uses HLS order
    
    # Convert to 0-255 range and combine into single integer
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    
    # Combine RGB values into single integer (format: 0xRRGGBB)
    rgb_int = (r_int << 16) | (g_int << 8) | b_int
    return rgb_int

def strided_palette(n: int, stride: int = 5) -> list[int]:
    """Generate a strided color palette.

    Args:
        n: Number of colors to generate.
        stride: Step size for color generation.

    Returns:
        List of strided color RGB integers.
    """
    strided_wheel = []
    wheel_palette = sns.color_palette("hls", n)
    i = 0
    while True:
        if not wheel_palette:
            break
        if i > len(wheel_palette)-1:
            i -= len(wheel_palette)
            continue
        hsl = wheel_palette.pop(i)
        rgb = hsl_to_rgb_int(hsl)
        strided_wheel.append(rgb)
        i += stride
    return strided_wheel


class ColorclassProcessor:
    """Processes Obsidian vault to add unique colorclass tags with NetworkX community detection."""

    # Available NetworkX community detection algorithms
    AVAILABLE_ALGORITHMS = {
        "louvain": "louvain_communities",
        "leiden": "leiden_communities",
        "greedy_modularity": "greedy_modularity_communities",
        "girvan_newman": "girvan_newman",
        "label_propagation": "asyn_lpa_communities",
        "kernighan_lin": "kernighan_lin_bisection",
    }

    def __init__(self, config_path: str | None = None):
        """Initialize processor with optional config file."""
        self.config = self._load_config(config_path)

    def _load_config(self, config_path: str | None) -> DictConfig:
        """Load configuration from YAML file or use defaults."""
        default_config = OmegaConf.create(
            {
                "colorclass_prefix": "colorclass",
                "dry_run": False,
                "backup_originals": False,
                "generate_graph_config": True,  # New option to control graph.json generation
                "community_detection": {
                    "algorithm": "louvain",  # Default algorithm
                    "algorithm_params": {  # Parameters passed to the algorithm
                        "seed": 42,
                        "resolution": 1.0,  # For louvain/leiden
                        "threshold": 1e-07,  # For leiden
                        "max_comm_size": 0,  # For leiden (0 = no limit)
                        "weight": None,  # Edge weight attribute name
                        "max_levels": None,  # For girvan_newman (None = all levels)
                    },
                    "min_community_size": 5,  # Minimum size for a community to get colorclass
                    "naming_scheme": "largest_node",  # 'cluster_id', 'largest_node', or 'sequential'
                },
            }
        )

        if config_path:
            config_path_obj = Path(config_path)
            if config_path_obj.exists():
                file_config = OmegaConf.load(config_path_obj)
                merged = OmegaConf.merge(default_config, file_config)
                # Convert to DictConfig if it's not already
                return (
                    OmegaConf.create(merged)
                    if not isinstance(merged, DictConfig)
                    else merged
                )
            else:
                logger.warning(f"Config file not found: {config_path}, using defaults")

        return default_config

    def list_algorithms(self) -> list[str]:
        """List available community detection algorithms."""
        available = []
        for algo_name, nx_func_name in self.AVAILABLE_ALGORITHMS.items():
            if hasattr(nx.community, nx_func_name):
                available.append(algo_name)
            else:
                logger.debug(
                    f"Algorithm {algo_name} ({nx_func_name}) not available in this NetworkX version"
                )
        return available

    def process_vault(
        self,
        vault_path: str,
        dry_run: bool | None = None,
        algorithm: str | None = None,
        #generate_graph_config: bool | None = True,
    ) -> dict[str, str]:
        """Process vault to add colorclass tags using community detection.

        Args:
            vault_path: Path to Obsidian vault directory
            dry_run: If True, show what would be changed without modifying files
            algorithm: Community detection algorithm to use (overrides config)

        Returns:
            Dictionary mapping article names to their assigned colorclass tags
        """
        vault_path_obj = Path(vault_path)
        dry_run = dry_run if dry_run is not None else self.config.dry_run
        algorithm = algorithm or self.config.community_detection.algorithm

        available_algorithms = self.list_algorithms()
        if algorithm not in available_algorithms:
            raise ValueError(
                f"Unknown or unavailable algorithm: {algorithm}. Available: {available_algorithms}"
            )

        logger.info(f"Processing vault: {vault_path_obj}")
        logger.info(f"Algorithm: {algorithm}")
        logger.info(f"Dry run: {dry_run}")

        # Load all documents
        _corpus = load_corpus(vault_path_obj)

        # Prune cruft
        # WARNING: this deletes content.
        corpus = []
        for doc in _corpus:
            if doc.tags and any(["prune" in tag for tag in doc.tags]):
                if doc.fpath:
                    doc.fpath.unlink()
                    logger.warning(f"Pruned {doc.title}")
                continue
            else:
                corpus.append(doc)

        # Run community detection on all documents
        communities, undirected_graph = self._detect_communities(corpus, algorithm)
        assignments = self._process_communities(communities, undirected_graph)

        if not assignments:
            logger.warning("No community assignments generated")
            return {}
        
        # Generate color palette for communities
        unique_colorclasses = list(set(assignments.values()))
        n = len(unique_colorclasses)
        palette = strided_palette(n)
        
        # Create colorclass to color mapping
        colorclass_to_color = dict(zip(unique_colorclasses, palette))

        # Apply changes to files
        if not dry_run:
            modified_count = self._apply_assignments(
                corpus, vault_path_obj, assignments
            )
            logger.success(f"Modified {modified_count} files")
            
            # Generate Obsidian graph configuration
            if self.config.generate_graph_config:
                self._generate_obsidian_graph_config(
                    vault_path_obj, colorclass_to_color
                )
        else:
            logger.info("Dry run complete - no files modified")
            if self.config.generate_graph_config:
                logger.info("Graph config would be generated with the following colorclasses:")
                for colorclass, color in colorclass_to_color.items():
                    logger.info(f"  {colorclass}: {color}")

        return assignments

    def _generate_obsidian_graph_config(
        self, vault_path: Path, colorclass_to_color: dict[str, int]
    ) -> None:
        """Generate or update .obsidian/graph.json with colorGroups."""
        obsidian_dir = vault_path / ".obsidian"
        graph_config_path = obsidian_dir / "graph.json"
        
        # Create .obsidian directory if it doesn't exist
        obsidian_dir.mkdir(exist_ok=True)
        
        # Default graph configuration
        default_config = {
            "collapse-filter": False,
            "search": "",
            "showTags": False,
            "showAttachments": False,
            "hideUnresolved": True,
            "showOrphans": False,
            "collapse-color-groups": True,
            "colorGroups": [],
            "collapse-display": True,
            "showArrow": False,
            "textFadeMultiplier": -2,
            "nodeSizeMultiplier": 0.64730224609375,
            "lineSizeMultiplier": 0.152437337239583,
            "collapse-forces": False,
            "centerStrength": 0.059814453125,
            "repelStrength": 15.6656901041667,
            "linkStrength": 1,
            "linkDistance": 30,
            "scale": 0.02849801640727639,
            "close": False
        }
        
        # Load existing configuration if it exists
        if graph_config_path.exists():
            try:
                with open(graph_config_path, 'r', encoding='utf-8') as f:
                    existing_config = json.load(f)
                # Merge with default config, preserving existing settings
                config = {**default_config, **existing_config}
            except (json.JSONDecodeError, FileNotFoundError) as e:
                logger.warning(f"Could not load existing graph config: {e}, using defaults")
                config = default_config
        else:
            config = default_config
        
        # Remove existing colorclass color groups to avoid duplicates
        prefix = f"tag:#{self.config.colorclass_prefix}/"
        existing_color_groups = config.get("colorGroups", [])
        filtered_color_groups = [
            group for group in existing_color_groups
            if not group.get("query", "").strip().startswith(prefix)
        ]
        
        # Generate new color groups for colorclasses
        new_color_groups = []
        for colorclass, rgb_color in colorclass_to_color.items():
            # Remove the prefix to get just the tag name
            tag_name = colorclass.replace(f"{self.config.colorclass_prefix}/", "")
            color_group = {
                "query": f"tag:#{self.config.colorclass_prefix}/{tag_name}",
                "color": {
                    "a": 1,
                    "rgb": rgb_color
                }
            }
            new_color_groups.append(color_group)
        
        # Combine filtered existing groups with new colorclass groups
        config["colorGroups"] = filtered_color_groups + new_color_groups
        
        # Write the updated configuration
        try:
            with open(graph_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            logger.success(f"Generated Obsidian graph config with {len(new_color_groups)} colorclass groups")
            logger.info(f"Graph config saved to: {graph_config_path}")
            
        except Exception as e:
            logger.error(f"Failed to write graph config: {e}")

    def _detect_communities(
        self, corpus: list["ObsDoc"], algorithm: str
    ) -> tuple[list[set[str]], nx.Graph]:
        """Use NetworkX community detection to assign colorclass tags."""
        logger.info(f"Starting community detection with {algorithm}...")

        # Build graph from corpus
        graph = build_graph(corpus)

        # Filter to existing documents only (no phantom nodes)
        existing_nodes = []
        for doc in corpus:
            if doc.node_name in graph.nodes:
                existing_nodes.append(doc.node_name)

        subgraph = graph.subgraph(existing_nodes).copy()
        logger.info(
            f"Clustering subgraph with {len(subgraph.nodes)} nodes, {len(subgraph.edges)} edges"
        )

        if len(subgraph.nodes) < 2:
            logger.warning("Graph too small for community detection")
            return []

        # Convert to undirected for algorithms
        undirected_graph = subgraph.to_undirected()

        # Run the selected algorithm
        communities = self._run_networkx_algorithm(undirected_graph, algorithm)

        if not communities:
            logger.error("Community detection failed to produce results")
            return []
        return communities, undirected_graph

    def _run_networkx_algorithm(
        self, graph: nx.Graph, algorithm: str
    ) -> list[set[str]]:
        """Run NetworkX community detection algorithm."""
        try:
            params = dict(self.config.community_detection.algorithm_params)
            nx_func_name = self.AVAILABLE_ALGORITHMS[algorithm]

            if not hasattr(nx.algorithms.community, nx_func_name):
                logger.error(
                    f"Algorithm {algorithm} ({nx_func_name}) not available in NetworkX"
                )
                return []

            func = getattr(nx.algorithms.community, nx_func_name)

            # Prepare parameters based on algorithm
            if algorithm == "louvain":
                nx_params = {}
                if "seed" in params and params["seed"] is not None:
                    nx_params["seed"] = params["seed"]
                if "resolution" in params and params["resolution"] is not None:
                    nx_params["resolution"] = params["resolution"]
                if "weight" in params and params["weight"] is not None:
                    nx_params["weight"] = params["weight"]

                logger.info(f"Running NetworkX Louvain with parameters: {nx_params}")
                communities = func(graph, **nx_params)

            elif algorithm == "leiden":
                nx_params = {}
                if "seed" in params and params["seed"] is not None:
                    nx_params["seed"] = params["seed"]
                if "resolution" in params and params["resolution"] is not None:
                    nx_params["resolution"] = params["resolution"]
                if "threshold" in params and params["threshold"] is not None:
                    nx_params["threshold"] = params["threshold"]
                if (
                    "max_comm_size" in params
                    and params["max_comm_size"] is not None
                    and params["max_comm_size"] > 0
                ):
                    nx_params["max_comm_size"] = params["max_comm_size"]

                logger.info(f"Running NetworkX Leiden with parameters: {nx_params}")
                communities = func(graph, **nx_params)

            elif algorithm == "greedy_modularity":
                nx_params = {}
                if "weight" in params and params["weight"] is not None:
                    nx_params["weight"] = params["weight"]
                if "resolution" in params and params["resolution"] is not None:
                    nx_params["resolution"] = params["resolution"]

                logger.info(
                    f"Running NetworkX Greedy Modularity with parameters: {nx_params}"
                )
                communities = func(graph, **nx_params)

            elif algorithm == "girvan_newman":
                nx_params = {}
                if "weight" in params and params["weight"] is not None:
                    nx_params["weight"] = params["weight"]

                logger.info(
                    f"Running NetworkX Girvan-Newman with parameters: {nx_params}"
                )
                # Girvan-Newman returns a generator of community divisions
                communities_gen = func(graph, **nx_params)

                # Get the best division (or up to max_levels)
                max_levels = params.get("max_levels", 10)  # Default to 10 levels
                if max_levels is None:
                    max_levels = 10

                best_communities = None
                best_modularity = -1

                for i, division in enumerate(communities_gen):
                    if i >= max_levels:
                        break
                    modularity = nx.algorithms.community.modularity(graph, division)
                    if modularity > best_modularity:
                        best_modularity = modularity
                        best_communities = division

                communities = best_communities if best_communities else []
                logger.info(f"Girvan-Newman best modularity: {best_modularity}")

            elif algorithm == "label_propagation":
                nx_params = {}
                if "seed" in params and params["seed"] is not None:
                    nx_params["seed"] = params["seed"]
                if "weight" in params and params["weight"] is not None:
                    nx_params["weight"] = params["weight"]

                logger.info(
                    f"Running NetworkX Label Propagation with parameters: {nx_params}"
                )
                communities = func(graph, **nx_params)

            elif algorithm == "kernighan_lin":
                # This is a bisection algorithm, so we'll apply it recursively
                logger.info("Running NetworkX Kernighan-Lin (recursive bisection)")
                communities = self._recursive_kernighan_lin(graph, params)

            else:
                logger.error(f"Unknown algorithm implementation: {algorithm}")
                return []

            if communities:
                communities_list = list(communities)
                logger.info(
                    f"NetworkX {algorithm} found {len(communities_list)} communities"
                )
                return communities_list
            else:
                logger.warning(f"NetworkX {algorithm} returned no communities")
                return []

        except Exception as e:
            logger.error(f"NetworkX {algorithm} failed: {e}")
            return []

    def _recursive_kernighan_lin(
        self, graph: nx.Graph, params: dict[str, Any], max_depth: int = 4
    ) -> list[set[str]]:
        """Apply Kernighan-Lin bisection recursively to create multiple communities."""
        communities: list[set[str]] = []

        def bisect_graph(g: nx.Graph, depth: int = 0) -> None:
            if len(g.nodes) < 4 or depth >= max_depth:  # Stop if too small or too deep
                communities.append(set(g.nodes))
                return

            try:
                # Apply Kernighan-Lin bisection
                partition = nx.algorithms.community.kernighan_lin_bisection(
                    g, seed=params.get("seed")
                )

                # If partition is successful and creates meaningful split
                if len(partition[0]) > 1 and len(partition[1]) > 1:
                    # Recursively bisect each partition
                    subgraph1 = g.subgraph(partition[0]).copy()
                    subgraph2 = g.subgraph(partition[1]).copy()
                    bisect_graph(subgraph1, depth + 1)
                    bisect_graph(subgraph2, depth + 1)
                else:
                    # Can't split meaningfully, add as single community
                    communities.append(set(g.nodes))
            except:
                # If bisection fails, add as single community
                communities.append(set(g.nodes))

        bisect_graph(graph)
        return communities

    def _process_communities(
        self, communities: list[set[str]], graph: nx.Graph
    ) -> dict[str, str]:
        """Process communities into colorclass assignments."""
        # Filter communities by minimum size
        min_size = self.config.community_detection.min_community_size
        filtered_communities = [
            community for community in communities if len(community) >= min_size
        ]

        logger.info(
            f"Found {len(communities)} communities, {len(filtered_communities)} after size filtering"
        )

        # Generate colorclass assignments
        assignments = {}
        naming_scheme = self.config.community_detection.naming_scheme

        for i, community in enumerate(filtered_communities):
            # Convert community to list if it's a set
            nodes = list(community)

            if naming_scheme == "cluster_id":
                # Use community index as colorclass name
                colorclass_tag = f"{self.config.colorclass_prefix}/cluster_{i}"

            elif naming_scheme == "largest_node":
                # Use the node with highest degree as colorclass name
                max_degree = -1
                representative_node = nodes[0]
                for node in nodes:
                    degree = graph.degree(node)
                    if degree > max_degree:
                        max_degree = degree
                        representative_node = node
                representative_node = representative_node.replace(" ", "-")

                colorclass_tag = (
                    f"{self.config.colorclass_prefix}/{representative_node}"
                )

            elif naming_scheme == "sequential":
                # Use sequential numbering
                colorclass_tag = f"{self.config.colorclass_prefix}/community_{i+1}"

            else:
                raise ValueError(f"Unknown naming scheme: {naming_scheme}")

            # Assign colorclass to all nodes in community
            for node in nodes:
                assignments[node] = colorclass_tag

            logger.info(f"Community {i} ({len(nodes)} nodes) → {colorclass_tag}")

        return assignments

    def _apply_assignments(
        self, corpus: list["ObsDoc"], vault_path: Path, assignments: dict[str, str]
    ) -> int:
        """Apply colorclass assignments to document files."""
        modified_count = 0

        for doc in corpus:
            if doc.node_name in assignments:
                colorclass_tag = assignments[doc.node_name]
                if self._add_colorclass_tag(doc, vault_path, colorclass_tag):
                    modified_count += 1

        return modified_count

    def _add_colorclass_tag(
        self, doc: "ObsDoc", vault_path: Path, colorclass_tag: str
    ) -> bool:
        """Add colorclass tag to a document's frontmatter.

        Args:
            doc: ObsDoc instance to modify
            vault_path: Path to vault directory
            colorclass_tag: The colorclass tag to add

        Returns:
            True if file was modified, False otherwise
        """
        file_path = doc.fpath if doc.fpath else vault_path / f"{doc.title}.md"

        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return False

        # Check if colorclass tag already exists
        existing_colorclass = None
        if doc.tags:
            for tag in doc.tags:
                if (tag is not None) and tag.startswith(
                    f"{self.config.colorclass_prefix}/"
                ):
                    existing_colorclass = tag
                    break

        if existing_colorclass == colorclass_tag:
            logger.debug(f"Colorclass tag already correct for {doc.title}")
            return False

        # Backup original if configured
        if self.config.backup_originals:
            backup_path = file_path.with_suffix(".md.bak")
            if not backup_path.exists():
                backup_path.write_text(file_path.read_text(encoding="utf-8"))

        # Read and parse content with python-frontmatter
        content = file_path.read_text(encoding="utf-8")
        post = frontmatter.loads(content)

        # Check if colorclass tag already exists
        existing_colorclass = None
        if post.metadata.get("tags"):
            for tag in post.metadata["tags"]:
                if (tag is not None) and tag.startswith(
                    f"{self.config.colorclass_prefix}/"
                ):
                    existing_colorclass = tag
                    break

        if existing_colorclass == colorclass_tag:
            logger.debug(f"Colorclass tag already correct for {doc.title}")
            return False

        # Backup original if configured
        if self.config.backup_originals:
            backup_path = file_path.with_suffix(".md.bak")
            if not backup_path.exists():
                backup_path.write_text(content)

        # Modify frontmatter
        if "tags" not in post.metadata:
            post.metadata["tags"] = []
        elif not isinstance(post.metadata["tags"], list):
            post.metadata["tags"] = [post.metadata["tags"]]

        # Remove existing colorclass tag if present
        post.metadata["tags"] = [
            tag
            for tag in post.metadata["tags"]
            if (tag is not None)
            and (not tag.startswith(f"{self.config.colorclass_prefix}/"))
        ]

        # Add new colorclass tag
        post.metadata["tags"].append(colorclass_tag)

        # Write modified content using frontmatter
        new_content = frontmatter.dumps(post)
        file_path.write_text(new_content, encoding="utf-8")
        logger.info(f"Added {colorclass_tag} to {doc.title}")
        return True

    def analyze_community_structure(
        self, vault_path: str, algorithm: str | None = None
    ) -> dict[str, Any]:
        """Analyze community structure that would be detected."""
        vault_path_obj = Path(vault_path)
        algorithm = algorithm or self.config.community_detection.algorithm

        available_algorithms = self.list_algorithms()
        if algorithm not in available_algorithms:
            raise ValueError(f"Unknown or unavailable algorithm: {algorithm}")

        logger.info(f"Analyzing community structure with algorithm: {algorithm}")

        corpus = load_corpus(vault_path_obj)
        graph = build_graph(corpus)

        # Filter to existing documents
        existing_nodes = [
            doc.node_name for doc in corpus if doc.node_name in graph.nodes
        ]
        subgraph = graph.subgraph(existing_nodes).to_undirected()

        if len(subgraph.nodes) < 2:
            return {"error": "Graph too small for analysis"}

        # Run community detection
        try:
            communities = self._run_networkx_algorithm(subgraph, algorithm)

            if not communities:
                return {"error": "Community detection failed"}

        except Exception as e:
            return {"error": f"Community detection failed: {e}"}

        # Calculate statistics
        community_sizes = [len(community) for community in communities]
        min_size = self.config.community_detection.min_community_size
        valid_communities = [
            community for community in communities if len(community) >= min_size
        ]

        # Calculate modularity for the detected communities
        try:
            modularity = nx.algorithms.community.modularity(subgraph, communities)
        except:
            modularity = None

        analysis = {
            "total_documents": len(corpus),
            "clustered_documents": len(existing_nodes),
            "total_communities": len(communities),
            "valid_communities": len(valid_communities),
            "modularity": modularity,
            "community_size_stats": {
                "min": min(community_sizes) if community_sizes else 0,
                "max": max(community_sizes) if community_sizes else 0,
                "mean": (
                    sum(community_sizes) / len(community_sizes)
                    if community_sizes
                    else 0
                ),
                "sizes": sorted(community_sizes, reverse=True)[:10],  # Top 10 sizes
            },
            "coverage": (
                len([n for community in valid_communities for n in community])
                / len(existing_nodes)
                if existing_nodes
                else 0
            ),
            "algorithm": algorithm,
        }

        logger.info(f"Community analysis: {analysis}")
        return analysis


def main() -> None:
    """CLI entry point for colorclass processor."""
    logger.add("colorclass_processor.log", rotation="1 MB")
    fire.Fire(ColorclassProcessor)


if __name__ == "__main__":
    main()


---
File: obsidian/graph.py
---
# obsidian/graph.py - Basic graph construction from documents

from collections import Counter
from pathlib import Path

import networkx as nx

from .parser import ObsDoc


def load_corpus(obs_root: Path | str) -> list[ObsDoc]:
    """Load all markdown documents from vault directory."""
    obs_root = Path(obs_root)
    return [ObsDoc.from_path(fpath) for fpath in obs_root.glob("*.md")]


def build_graph(corpus: list[ObsDoc]) -> nx.DiGraph:
    """Build directed graph from document corpus."""
    G = nx.DiGraph()
    std_titles = []

    for doc in corpus:
        src = doc.title.lower()
        std_titles.append(src)
        edges = [(src, tgt) for tgt in doc.links]
        G.add_edges_from(edges)

    # Mark which nodes actually exist vs are just linked to
    nx.set_node_attributes(G, False, "exists")
    nx.set_node_attributes(G, {title: True for title in std_titles}, "exists")

    return G


def get_link_statistics(corpus: list[ObsDoc]) -> tuple[Counter[str], Counter[str]]:
    """Calculate tag and indegree statistics."""
    tags: Counter[str] = Counter()
    indegree: Counter[str] = Counter()

    for doc in corpus:
        if doc.tags:
            tags.update(doc.tags)
        if doc.links:
            indegree.update(doc.links)

    return tags, indegree


def find_candidates(G: nx.DiGraph) -> dict[str, int]:
    """Find missing documents with their connection degrees."""
    candidates = [
        node
        for node, exists in nx.get_node_attributes(G, "exists").items()
        if not exists
    ]

    GU = G.to_undirected()
    cand_degree = {}
    for cand in candidates:
        cand_degree[cand] = len(list(nx.neighbors(GU, cand)))

    return cand_degree



---
File: obsidian/parser.py
---
# obsidian/parser.py - Core document parsing utilities

import re
from pathlib import Path
from typing import Any

import frontmatter


def get_wikilinks(text: str) -> list[str]:
    """Extract all wikilinks from text using [[link]] pattern."""
    links_pat = re.compile(r"\[\[(.+?)\]\]")
    return re.findall(links_pat, text)


def clean_links(wikilinks: list[str], collect_aliases: bool = False) -> list[str]:
    """Canonicalize aliases, standardize case."""
    if collect_aliases:
        raise NotImplementedError
    outv = []
    for link in wikilinks:
        if "|" in link:
            try:
                link, alias = link.split("|")
            except:
                # print(link)
                # raise
                continue  # fuck it
        outv.append(link.lower())
    return outv


class ObsDoc:
    """Represents a single Obsidian document."""

    def __init__(self, title: str, raw: str, fpath: Path | str | None = None):
        self.title = title
        self.raw = raw

        # Parse with python-frontmatter
        self.post = frontmatter.loads(raw)
        self.frontmatter = self.post.metadata
        self.body = self.post.content

        # Use frontmatter title if available
        if "title" in self.frontmatter:
            self.title = self.frontmatter["title"]

        self.links = clean_links(get_wikilinks(self.body))
        self.tags = self.frontmatter.get("tags", [])
        self.fpath = Path(fpath) if fpath else None

    @property
    def node_name(self) -> str:
        """Canonicalized title for graph nodes."""
        return self.title.lower()

    @classmethod
    def from_path(cls, fpath: Path | str) -> "ObsDoc":
        """Create ObsDoc from file path."""
        fpath = Path(fpath)
        with fpath.open(encoding="utf-8") as f:
            try:
                return cls(fpath.stem, f.read(), fpath=fpath)
            except Exception as e:
                print(fpath)
                raise e


# Keep these for backwards compatibility
def read_yaml(txt: str) -> dict[str, Any]:
    """Parse YAML text and return as dictionary."""
    import yaml

    result = yaml.load(txt, yaml.Loader)
    return result if isinstance(result, dict) else {}


def extract_frontmatter(doc: str) -> tuple[dict[str, Any], str]:
    """Extract YAML frontmatter and body from markdown document."""
    post = frontmatter.loads(doc)
    return post.metadata, post.content



---
File: pyproject.toml
---
# pyproject.toml - Modern Python packaging configuration

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "dtd-obsidian-tools"
version = "0.1.2"
description = "Tools for working with Obsidian vaults, including community detection and graph analysis"
readme = "README.md"
license = {file = "LICENSE"}
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]
maintainers = [
    {name = "Your Name", email = "your.email@example.com"}
]
keywords = ["obsidian", "knowledge-graph", "community-detection", "networkx", "markdown"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: End Users/Desktop",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Text Processing :: Markup",
    "Topic :: Scientific/Engineering :: Information Analysis",
    "Topic :: Office/Business :: Groupware",
]
requires-python = ">=3.10"
dependencies = [
    "networkx>=3.0",
    "loguru>=0.7.0",
    "fire>=0.5.0",
    "omegaconf>=2.3.0",
    "pyyaml>=6.0",
    "python-frontmatter>=1.0.0",
    "seaborn>=0.11.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "black>=23.0",
    "isort>=5.12",
    "flake8>=6.0",
    "mypy>=1.0",
    "pre-commit>=3.0",
]
docs = [
    "mkdocs>=1.4",
    "mkdocs-material>=9.0",
    "mkdocstrings[python]>=0.20",
]

[project.urls]
Homepage = "https://github.com/yourusername/dtd-obsidian-tools"
Documentation = "https://github.com/yourusername/dtd-obsidian-tools#readme"
Repository = "https://github.com/yourusername/dtd-obsidian-tools"
"Bug Tracker" = "https://github.com/yourusername/dtd-obsidian-tools/issues"

[project.scripts]
obsidian-colorclass = "obsidian.colorclass_processor:main"

[tool.setuptools.packages.find]
include = ["obsidian*"]

[tool.setuptools.package-data]
obsidian = ["*.yml", "*.yaml"]

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["obsidian"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = [
    "networkx.*",
    "yaml.*",
    "fire.*",
    "omegaconf.*",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
    "--cov=obsidian",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
]

[tool.coverage.run]
source = ["obsidian"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]



---
File: requirements.txt
---



---
File: tests/test_colorclass_processor.py
---
# tests/test_colorclass_processor.py - Test colorclass processor functionality

import json
import tempfile
from pathlib import Path

import pytest
from obsidian.colorclass_processor import ColorclassProcessor, hsl_to_rgb_int, strided_palette
from obsidian.parser import ObsDoc


def test_hsl_to_rgb_int():
    """Test HSL to RGB integer conversion."""
    # Red (hue=0, sat=1, light=0.5)
    rgb = hsl_to_rgb_int((0.0, 1.0, 0.5))
    assert rgb == 0xFF0000  # Red
    
    # Blue (hue=0.67, sat=1, light=0.5)
    rgb = hsl_to_rgb_int((0.67, 1.0, 0.5))
    assert isinstance(rgb, int)
    assert 0 <= rgb <= 0xFFFFFF


def test_strided_palette():
    """Test strided color palette generation."""
    colors = strided_palette(5)
    assert len(colors) == 5
    assert all(isinstance(c, int) for c in colors)
    assert all(0 <= c <= 0xFFFFFF for c in colors)


def test_processor_init():
    """Test processor initialization."""
    processor = ColorclassProcessor()
    assert processor.config.colorclass_prefix == "colorclass"
    assert processor.config.dry_run is False


def test_list_algorithms():
    """Test algorithm listing."""
    processor = ColorclassProcessor()
    algorithms = processor.list_algorithms()
    assert isinstance(algorithms, list)
    assert "louvain" in algorithms  # Should be available in most NetworkX versions


def test_process_vault_dry_run():
    """Test vault processing in dry run mode."""
    processor = ColorclassProcessor()
    
    with tempfile.TemporaryDirectory() as tmpdir:
        vault_path = Path(tmpdir)
        
        # Create test documents
        (vault_path / "doc1.md").write_text("""---
title: Document 1
tags: [test]
---
# Document 1
Links to [[Document 2]].""")
        
        (vault_path / "doc2.md").write_text("""---
title: Document 2
tags: [test]
---
# Document 2
Links to [[Document 1]].""")
        
        # Process in dry run mode
        assignments = processor.process_vault(str(vault_path), dry_run=True)
        
        # Should return assignments without modifying files
        assert isinstance(assignments, dict)
        # Original files should be unchanged
        content1 = (vault_path / "doc1.md").read_text()
        assert "colorclass/" not in content1


def test_analyze_community_structure():
    """Test community structure analysis."""
    processor = ColorclassProcessor()
    
    with tempfile.TemporaryDirectory() as tmpdir:
        vault_path = Path(tmpdir)
        
        # Create minimal test vault
        (vault_path / "doc1.md").write_text("# Doc1\nLinks to [[doc2]].")
        (vault_path / "doc2.md").write_text("# Doc2\nLinks to [[doc1]].")
        
        analysis = processor.analyze_community_structure(str(vault_path))
        
        assert "total_documents" in analysis
        assert "clustered_documents" in analysis
        assert "total_communities" in analysis
        assert analysis["total_documents"] == 2
        assert analysis["clustered_documents"] == 2


def test_config_loading():
    """Test configuration loading with custom config."""
    with tempfile.TemporaryDirectory() as tmpdir:
        config_path = Path(tmpdir) / "config.yml"
        config_path.write_text("""
colorclass_prefix: "custom"
dry_run: true
community_detection:
  algorithm: "greedy_modularity"
  min_community_size: 3
""")
        
        processor = ColorclassProcessor(str(config_path))
        assert processor.config.colorclass_prefix == "custom"
        assert processor.config.dry_run is True
        assert processor.config.community_detection.algorithm == "greedy_modularity"
        assert processor.config.community_detection.min_community_size == 3


---
File: tests/test_graph.py
---
# tests/test_graph.py - Test graph construction functionality

import networkx as nx
import pytest
from pathlib import Path
from obsidian.graph import build_graph, get_link_statistics, find_candidates
from obsidian.parser import ObsDoc


def test_build_graph_empty():
    """Test graph building with empty corpus."""
    corpus = []
    graph = build_graph(corpus)
    assert len(graph.nodes) == 0
    assert len(graph.edges) == 0


def test_build_graph_single_doc():
    """Test graph with single document with no links."""
    doc = ObsDoc("test", "# Test\nNo links here.")
    corpus = [doc]
    graph = build_graph(corpus)
    
    # No links means no nodes are added to the graph
    assert len(graph.nodes) == 0
    assert len(graph.edges) == 0


def test_build_graph_with_links():
    """Test graph with linked documents."""
    doc1 = ObsDoc("doc1", "# Doc1\nLinks to [[doc2]] and [[missing]].")
    doc2 = ObsDoc("doc2", "# Doc2\nLinks to [[doc1]].")
    corpus = [doc1, doc2]
    
    graph = build_graph(corpus)
    
    # Should have 3 nodes: doc1, doc2, missing (phantom)
    assert len(graph.nodes) == 3
    assert "doc1" in graph.nodes
    assert "doc2" in graph.nodes
    assert "missing" in graph.nodes
    
    # Check existence attributes
    assert graph.nodes["doc1"]["exists"] is True
    assert graph.nodes["doc2"]["exists"] is True
    assert graph.nodes["missing"]["exists"] is False
    
    # Check edges
    assert graph.has_edge("doc1", "doc2")
    assert graph.has_edge("doc1", "missing")
    assert graph.has_edge("doc2", "doc1")


def test_get_link_statistics():
    """Test link and tag statistics calculation."""
    doc1 = ObsDoc("doc1", """---
tags: [python, test]
---
# Doc1
Links to [[doc2]].""")
    
    doc2 = ObsDoc("doc2", """---
tags: [python, example]
---
# Doc2
Links to [[doc1]] and [[doc2]].""")
    
    corpus = [doc1, doc2]
    tags, indegree = get_link_statistics(corpus)
    
    # Tag counts
    assert tags["python"] == 2
    assert tags["test"] == 1
    assert tags["example"] == 1
    
    # Link counts (indegree)
    assert indegree["doc2"] == 2  # linked from doc1 and doc2
    assert indegree["doc1"] == 1  # linked from doc2


def test_find_candidates():
    """Test finding missing documents."""
    doc1 = ObsDoc("existing", "# Existing\nLinks to [[missing1]] and [[missing2]].")
    corpus = [doc1]
    
    graph = build_graph(corpus)
    candidates = find_candidates(graph)
    
    assert "missing1" in candidates
    assert "missing2" in candidates
    assert "existing" not in candidates
    
    # Each missing doc should have degree 1
    assert candidates["missing1"] == 1
    assert candidates["missing2"] == 1


---
File: tests/test_integration.py
---
# tests/test_integration.py - Basic integration tests

import tempfile
from pathlib import Path

import pytest
from obsidian import load_corpus, build_graph, ObsDoc


def test_load_corpus_empty_directory():
    """Test loading corpus from empty directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        vault_path = Path(tmpdir)
        corpus = load_corpus(vault_path)
        assert len(corpus) == 0


def test_load_corpus_with_files():
    """Test loading corpus with markdown files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        vault_path = Path(tmpdir)
        
        # Create test files
        (vault_path / "doc1.md").write_text("# Document 1")
        (vault_path / "doc2.md").write_text("# Document 2")
        (vault_path / "not_markdown.txt").write_text("Not a markdown file")
        
        corpus = load_corpus(vault_path)
        
        # Should only load .md files
        assert len(corpus) == 2
        assert all(isinstance(doc, ObsDoc) for doc in corpus)
        titles = [doc.title for doc in corpus]
        assert "doc1" in titles
        assert "doc2" in titles


def test_full_workflow():
    """Test complete workflow from corpus loading to graph building."""
    with tempfile.TemporaryDirectory() as tmpdir:
        vault_path = Path(tmpdir)
        
        # Create interconnected documents
        (vault_path / "home.md").write_text("""---
title: Home Page
tags: [index, main]
---
# Welcome
See [[Projects]] and [[Ideas]].""")
        
        (vault_path / "projects.md").write_text("""---
title: Projects
tags: [work]
---
# My Projects
Back to [[Home Page]].""")
        
        (vault_path / "ideas.md").write_text("""---
title: Ideas
tags: [creative, brainstorm]
---
# Random Ideas
Link to [[Projects]] and [[Home Page]].""")
        
        # Load and process
        corpus = load_corpus(vault_path)
        graph = build_graph(corpus)
        
        # Verify corpus
        assert len(corpus) == 3
        
        # Verify graph structure
        assert len(graph.nodes) >= 3
        assert "home page" in graph.nodes
        assert "projects" in graph.nodes
        assert "ideas" in graph.nodes
        
        # Verify connections
        assert graph.has_edge("home page", "projects")
        assert graph.has_edge("home page", "ideas")
        assert graph.has_edge("projects", "home page")
        assert graph.has_edge("ideas", "projects")
        assert graph.has_edge("ideas", "home page")
        
        # Verify node attributes
        for title in ["home page", "projects", "ideas"]:
            assert graph.nodes[title]["exists"] is True


---
File: tests/test_parser.py
---
# tests/test_parser.py - Test core parsing functionality

from pathlib import Path

import frontmatter
import pytest

from obsidian.parser import ObsDoc, clean_links, extract_frontmatter, get_wikilinks


def test_extract_frontmatter():
    """Test YAML frontmatter extraction using python-frontmatter."""
    doc = """---
title: Test Document
tags: [tag1, tag2]
---

# Content here
Some [[link]] content."""

    metadata, body = extract_frontmatter(doc)
    assert metadata["title"] == "Test Document"
    assert metadata["tags"] == ["tag1", "tag2"]
    assert "# Content here" in body


def test_extract_frontmatter_no_frontmatter():
    """Test document without frontmatter."""
    doc = "# Just content\nNo frontmatter here."
    metadata, body = extract_frontmatter(doc)
    assert metadata == {}
    assert body == doc


def test_get_wikilinks():
    """Test wikilink extraction."""
    text = "Here are some [[Link One]] and [[Link Two|Alias]] links."
    links = get_wikilinks(text)
    assert links == ["Link One", "Link Two|Alias"]


def test_clean_links():
    """Test link cleaning and canonicalization."""
    links = ["Link One", "Link Two|Alias", "UPPERCASE"]
    cleaned = clean_links(links)
    assert cleaned == ["link one", "link two", "uppercase"]


def test_obsdoc_creation():
    """Test ObsDoc creation and properties."""
    content = """---
title: My Document
tags: [test, example]
---

# My Document
This has a [[wikilink]] and [[Another Link|alias]].
"""

    doc = ObsDoc("test-doc", content)
    assert doc.title == "My Document"  # Should use frontmatter title
    assert doc.tags == ["test", "example"]
    assert doc.links == ["wikilink", "another link"]
    assert doc.node_name == "my document"


def test_obsdoc_no_frontmatter():
    """Test ObsDoc with no frontmatter."""
    content = "# Simple Doc\nJust content with [[a link]]."
    doc = ObsDoc("simple-doc", content)
    assert doc.title == "simple-doc"
    assert doc.tags == []
    assert doc.links == ["a link"]


def test_frontmatter_roundtrip():
    """Test that frontmatter can be read and written back correctly."""
    content = """---
title: Test
tags: [one, two]
date: 2025-01-01
---

Body content here."""

    # Parse with frontmatter
    post = frontmatter.loads(content)
    assert post.metadata["title"] == "Test"
    assert post.metadata["tags"] == ["one", "two"]
    assert post.content.strip() == "Body content here."

    # Roundtrip test
    reconstructed = frontmatter.dumps(post)
    post2 = frontmatter.loads(reconstructed)
    assert post2.metadata == post.metadata
    assert post2.content == post.content


